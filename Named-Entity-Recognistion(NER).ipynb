{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-08T08:03:57.661566Z","iopub.execute_input":"2023-03-08T08:03:57.662217Z","iopub.status.idle":"2023-03-08T08:03:57.702185Z","shell.execute_reply.started":"2023-03-08T08:03:57.662168Z","shell.execute_reply":"2023-03-08T08:03:57.700719Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/named-entity-recognistionner/NER.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:28:47.102212Z","iopub.execute_input":"2023-03-08T08:28:47.102524Z","iopub.status.idle":"2023-03-08T08:28:55.873023Z","shell.execute_reply.started":"2023-03-08T08:28:47.102492Z","shell.execute_reply":"2023-03-08T08:28:55.871928Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/named-entity-recognistionner/NER.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:28:55.875243Z","iopub.execute_input":"2023-03-08T08:28:55.876344Z","iopub.status.idle":"2023-03-08T08:28:56.512049Z","shell.execute_reply.started":"2023-03-08T08:28:55.876302Z","shell.execute_reply":"2023-03-08T08:28:56.511055Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"    Sentence #           Word  POS Tag\n0  Sentence: 1      Thousands  NNS   O\n1          NaN             of   IN   O\n2          NaN  demonstrators  NNS   O\n3          NaN           have  VBP   O\n4          NaN        marched  VBN   O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 1</td>\n      <td>Thousands</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>of</td>\n      <td>IN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>demonstrators</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>have</td>\n      <td>VBP</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>marched</td>\n      <td>VBN</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"- Words are broken into columns, which are our feature X.\n-  And the tag column will be our label Y.","metadata":{}},{"cell_type":"code","source":"def get_map(data, token_or_a_tag):\n    tok2indx = {}\n    idx2tok = {}\n    vocab = []\n    if token_or_a_tag == 'token':\n        vocab = list(set(data['Word'].to_list())) # Set to remove the duplicates from the list \n    else:\n        vocab = list(set(data['Tag'].to_list()))\n        print(vocab)\n    \n    \n    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)} #iterators over the vocab and generates pairs \n    idx2idx = {idx:tok for  idx, tok in enumerate(vocab)}\n    return tok2idx, idx2tok","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:28:56.513476Z","iopub.execute_input":"2023-03-08T08:28:56.513986Z","iopub.status.idle":"2023-03-08T08:28:56.521430Z","shell.execute_reply.started":"2023-03-08T08:28:56.513943Z","shell.execute_reply":"2023-03-08T08:28:56.520230Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"token2idx, idx2token = get_map(data, 'token')\ntag2idx, idx2tag = get_map(data,'tag')\ntag2idx","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:28:56.524921Z","iopub.execute_input":"2023-03-08T08:28:56.525457Z","iopub.status.idle":"2023-03-08T08:28:56.665958Z","shell.execute_reply.started":"2023-03-08T08:28:56.525379Z","shell.execute_reply":"2023-03-08T08:28:56.664607Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['I-tim', 'B-gpe', 'I-gpe', 'I-eve', 'I-geo', 'O', 'B-per', 'I-org', 'B-tim', 'B-nat', 'B-org', 'I-art', 'I-per', 'B-geo', 'I-nat', 'B-eve', 'B-art']\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'I-tim': 0,\n 'B-gpe': 1,\n 'I-gpe': 2,\n 'I-eve': 3,\n 'I-geo': 4,\n 'O': 5,\n 'B-per': 6,\n 'I-org': 7,\n 'B-tim': 8,\n 'B-nat': 9,\n 'B-org': 10,\n 'I-art': 11,\n 'I-per': 12,\n 'B-geo': 13,\n 'I-nat': 14,\n 'B-eve': 15,\n 'B-art': 16}"},"metadata":{}}]},{"cell_type":"code","source":"tag2idx","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:28:56.667598Z","iopub.execute_input":"2023-03-08T08:28:56.668345Z","iopub.status.idle":"2023-03-08T08:28:56.694145Z","shell.execute_reply.started":"2023-03-08T08:28:56.668276Z","shell.execute_reply":"2023-03-08T08:28:56.688191Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'I-tim': 0,\n 'B-gpe': 1,\n 'I-gpe': 2,\n 'I-eve': 3,\n 'I-geo': 4,\n 'O': 5,\n 'B-per': 6,\n 'I-org': 7,\n 'B-tim': 8,\n 'B-nat': 9,\n 'B-org': 10,\n 'I-art': 11,\n 'I-per': 12,\n 'B-geo': 13,\n 'I-nat': 14,\n 'B-eve': 15,\n 'B-art': 16}"},"metadata":{}}]},{"cell_type":"code","source":"data['Word_idx'] = data['Word'].map(token2idx)\ndata['Tag_idx'] = data['Tag'].map(tag2idx)\ndata_fillna = data.fillna(method='ffill', axis=0)\n# Groupby and collect columns\n# Sentence # column is not used as the index of the resulting data_group dataframe.\ndata_group = data_fillna.groupby(['Sentence #'],as_index=False)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n# all the words in the sentence, their corresponding POS tags, their original tags, their word indices, and their tag indices. ","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:28:56.697968Z","iopub.execute_input":"2023-03-08T08:28:56.699037Z","iopub.status.idle":"2023-03-08T08:29:00.504755Z","shell.execute_reply.started":"2023-03-08T08:28:56.698981Z","shell.execute_reply":"2023-03-08T08:29:00.503526Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  \n","output_type":"stream"}]},{"cell_type":"code","source":"data_group","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:29:00.506433Z","iopub.execute_input":"2023-03-08T08:29:00.507028Z","iopub.status.idle":"2023-03-08T08:29:00.547998Z","shell.execute_reply.started":"2023-03-08T08:29:00.506987Z","shell.execute_reply":"2023-03-08T08:29:00.547056Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"            Sentence #                                               Word  \\\n0          Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n1         Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n2        Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n3       Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n4      Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n...                ...                                                ...   \n47954   Sentence: 9995  [Opposition, leader, Mir, Hossein, Mousavi, ha...   \n47955   Sentence: 9996  [On, Thursday, ,, Iranian, state, media, publi...   \n47956   Sentence: 9997  [Following, Iran, 's, disputed, June, 12, elec...   \n47957   Sentence: 9998  [Since, then, ,, authorities, have, held, publ...   \n47958   Sentence: 9999  [The, United, Nations, is, praising, the, use,...   \n\n                                                     POS  \\\n0      [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n1      [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n2      [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n3         [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n4      [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n...                                                  ...   \n47954  [NNP, NN, NNP, NNP, NNP, VBZ, VBN, PRP, VBZ, T...   \n47955  [IN, NNP, ,, JJ, NN, NNS, VBN, DT, NN, IN, DT,...   \n47956  [VBG, NNP, POS, JJ, NNP, CD, NNS, ,, NNS, NNS,...   \n47957  [IN, RB, ,, NNS, VBP, VBN, JJ, NNS, IN, DT, VB...   \n47958  [DT, NNP, NNP, VBZ, VBG, DT, NN, IN, JJ, NNS, ...   \n\n                                                     Tag  \\\n0      [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...   \n1      [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n2      [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...   \n3                      [O, O, O, O, O, O, O, O, O, O, O]   \n4      [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...   \n...                                                  ...   \n47954  [O, O, O, B-per, I-per, O, O, O, O, O, O, O, O...   \n47955  [O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...   \n47956  [O, B-geo, O, O, B-tim, I-tim, O, O, O, O, O, ...   \n47957  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n47958  [O, B-org, I-org, O, O, O, O, O, O, O, O, O, O...   \n\n                                                Word_idx  \\\n0      [3290, 18254, 1317, 23803, 29599, 11879, 31142...   \n1      [27174, 11716, 26179, 15498, 19043, 1800, 2607...   \n2      [17398, 6071, 29966, 31432, 19924, 7543, 16477...   \n3      [25436, 14325, 16123, 7687, 33794, 136, 26167,...   \n4      [19055, 4687, 5343, 26508, 18895, 21857, 31523...   \n...                                                  ...   \n47954  [5478, 17083, 9845, 25619, 30522, 31815, 21857...   \n47955  [22349, 32072, 6497, 27174, 24496, 28663, 1218...   \n47956  [27744, 33821, 17832, 27898, 5267, 13550, 2148...   \n47957  [21126, 7216, 6497, 3897, 23803, 2592, 34413, ...   \n47958  [6393, 13898, 32270, 24436, 10468, 13121, 2003...   \n\n                                                 Tag_idx  \n0      [5, 5, 5, 5, 5, 5, 13, 5, 5, 5, 5, 5, 13, 5, 5...  \n1      [1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n2      [5, 5, 8, 5, 5, 5, 5, 5, 13, 5, 5, 5, 5, 5, 10...  \n3                      [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]  \n4      [13, 5, 5, 6, 12, 5, 8, 5, 13, 5, 1, 5, 1, 5, ...  \n...                                                  ...  \n47954  [5, 5, 5, 6, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,...  \n47955  [5, 8, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 10, 7, 5,...  \n47956  [5, 13, 5, 5, 8, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5,...  \n47957  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n47958  [5, 10, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,...  \n\n[47959 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Tag</th>\n      <th>Word_idx</th>\n      <th>Tag_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 1</td>\n      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n      <td>[3290, 18254, 1317, 23803, 29599, 11879, 31142...</td>\n      <td>[5, 5, 5, 5, 5, 5, 13, 5, 5, 5, 5, 5, 13, 5, 5...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sentence: 10</td>\n      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n      <td>[27174, 11716, 26179, 15498, 19043, 1800, 2607...</td>\n      <td>[1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sentence: 100</td>\n      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n      <td>[17398, 6071, 29966, 31432, 19924, 7543, 16477...</td>\n      <td>[5, 5, 8, 5, 5, 5, 5, 5, 13, 5, 5, 5, 5, 5, 10...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sentence: 1000</td>\n      <td>[They, left, after, a, tense, hour-long, stand...</td>\n      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n      <td>[25436, 14325, 16123, 7687, 33794, 136, 26167,...</td>\n      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sentence: 10000</td>\n      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n      <td>[19055, 4687, 5343, 26508, 18895, 21857, 31523...</td>\n      <td>[13, 5, 5, 6, 12, 5, 8, 5, 13, 5, 1, 5, 1, 5, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>47954</th>\n      <td>Sentence: 9995</td>\n      <td>[Opposition, leader, Mir, Hossein, Mousavi, ha...</td>\n      <td>[NNP, NN, NNP, NNP, NNP, VBZ, VBN, PRP, VBZ, T...</td>\n      <td>[O, O, O, B-per, I-per, O, O, O, O, O, O, O, O...</td>\n      <td>[5478, 17083, 9845, 25619, 30522, 31815, 21857...</td>\n      <td>[5, 5, 5, 6, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,...</td>\n    </tr>\n    <tr>\n      <th>47955</th>\n      <td>Sentence: 9996</td>\n      <td>[On, Thursday, ,, Iranian, state, media, publi...</td>\n      <td>[IN, NNP, ,, JJ, NN, NNS, VBN, DT, NN, IN, DT,...</td>\n      <td>[O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...</td>\n      <td>[22349, 32072, 6497, 27174, 24496, 28663, 1218...</td>\n      <td>[5, 8, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 10, 7, 5,...</td>\n    </tr>\n    <tr>\n      <th>47956</th>\n      <td>Sentence: 9997</td>\n      <td>[Following, Iran, 's, disputed, June, 12, elec...</td>\n      <td>[VBG, NNP, POS, JJ, NNP, CD, NNS, ,, NNS, NNS,...</td>\n      <td>[O, B-geo, O, O, B-tim, I-tim, O, O, O, O, O, ...</td>\n      <td>[27744, 33821, 17832, 27898, 5267, 13550, 2148...</td>\n      <td>[5, 13, 5, 5, 8, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5,...</td>\n    </tr>\n    <tr>\n      <th>47957</th>\n      <td>Sentence: 9998</td>\n      <td>[Since, then, ,, authorities, have, held, publ...</td>\n      <td>[IN, RB, ,, NNS, VBP, VBN, JJ, NNS, IN, DT, VB...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[21126, 7216, 6497, 3897, 23803, 2592, 34413, ...</td>\n      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n    </tr>\n    <tr>\n      <th>47958</th>\n      <td>Sentence: 9999</td>\n      <td>[The, United, Nations, is, praising, the, use,...</td>\n      <td>[DT, NNP, NNP, VBZ, VBG, DT, NN, IN, JJ, NNS, ...</td>\n      <td>[O, B-org, I-org, O, O, O, O, O, O, O, O, O, O...</td>\n      <td>[6393, 13898, 32270, 24436, 10468, 13121, 2003...</td>\n      <td>[5, 10, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>47959 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def preprocing_the_data(data_group, data):\n    #get max token and tag length\n    n_token = len(list(set(data['Word'].to_list())))\n    n_tag = len(list(set(data['Tag'].to_list())))\n\n    #Pad tokens (X var)    \n    tokens = data_group['Word_idx'].tolist()\n    maxlen = max([len(s) for s in tokens])\n    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n\n    #Pad Tags (y var) and convert it into one hot encoding\n    tags = data_group['Tag_idx'].tolist()\n    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx['O'])\n    n_tags = len(tag2idx)\n    print(tag2idx[\"O\"] )\n    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n    \n    #Split train, test and validation set\n    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n\n    print('\\ntrain_tokens length:', len(train_tokens))\n    print('\\ntrain_tags length:', len(train_tags))\n    print('\\ntest_tokens length:', len(test_tokens))\n    print('\\ntest_tags:', len(test_tags))\n    print('\\nval_tokens:', len(val_tokens))\n    print('\\nval_tags:', len(val_tags))\n    \n    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n\ntrain_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = preprocing_the_data(data_group, data)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:29:00.550777Z","iopub.execute_input":"2023-03-08T08:29:00.551044Z","iopub.status.idle":"2023-03-08T08:29:01.558625Z","shell.execute_reply.started":"2023-03-08T08:29:00.551018Z","shell.execute_reply":"2023-03-08T08:29:01.557405Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"5\n\ntrain_tokens length: 32372\n\ntrain_tags length: 32372\n\ntest_tokens length: 4796\n\ntest_tags: 4796\n\nval_tokens: 10791\n\nval_tags: 10791\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow\nfrom tensorflow.keras import Sequential, Model, Input\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom tensorflow.keras.utils import plot_model\nfrom numpy.random import seed\nseed(1)\ntensorflow.random.set_seed(2)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:29:01.560870Z","iopub.execute_input":"2023-03-08T08:29:01.561837Z","iopub.status.idle":"2023-03-08T08:29:01.571059Z","shell.execute_reply.started":"2023-03-08T08:29:01.561799Z","shell.execute_reply":"2023-03-08T08:29:01.569907Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"input_dim = len(list(set(data['Word'].to_list())))+1\noutput_dim = 64\ninput_length = max([len(s) for s in data_group['Word_idx'].tolist()])\nn_tags = len(tag2idx)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:29:01.575124Z","iopub.execute_input":"2023-03-08T08:29:01.575835Z","iopub.status.idle":"2023-03-08T08:29:01.636840Z","shell.execute_reply.started":"2023-03-08T08:29:01.575792Z","shell.execute_reply":"2023-03-08T08:29:01.635841Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_bilstm_lstm_model():\n    model = Sequential()\n\n    # Add Embedding layer\n    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n\n    # Add bidirectional LSTM\n    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n\n    # Add LSTM\n    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n\n    # Add timeDistributed Layer\n    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n\n    #Optimiser \n    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:29:01.638448Z","iopub.execute_input":"2023-03-08T08:29:01.638885Z","iopub.status.idle":"2023-03-08T08:29:01.648130Z","shell.execute_reply.started":"2023-03-08T08:29:01.638842Z","shell.execute_reply":"2023-03-08T08:29:01.646137Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def train_model(X, y, model):\n    loss = list()\n    for i in range(25):\n        # fit model for one epoch on this sequence\n        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n        loss.append(hist.history['loss'][0])\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:29:01.650597Z","iopub.execute_input":"2023-03-08T08:29:01.652306Z","iopub.status.idle":"2023-03-08T08:29:01.661885Z","shell.execute_reply.started":"2023-03-08T08:29:01.652267Z","shell.execute_reply":"2023-03-08T08:29:01.660773Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"results = pd.DataFrame()\nmodel_bilstm_lstm = get_bilstm_lstm_model()\nplot_model(model_bilstm_lstm)\nresults['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:29:01.663544Z","iopub.execute_input":"2023-03-08T08:29:01.664039Z","iopub.status.idle":"2023-03-08T08:46:26.673054Z","shell.execute_reply.started":"2023-03-08T08:29:01.663998Z","shell.execute_reply":"2023-03-08T08:46:26.671885Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 104, 64)           2251072   \n                                                                 \n bidirectional (Bidirectiona  (None, 104, 128)         66048     \n l)                                                              \n                                                                 \n lstm_1 (LSTM)               (None, 104, 64)           49408     \n                                                                 \n time_distributed (TimeDistr  (None, 104, 17)          1105      \n ibuted)                                                         \n                                                                 \n=================================================================\nTotal params: 2,367,633\nTrainable params: 2,367,633\nNon-trainable params: 0\n_________________________________________________________________\n26/26 [==============================] - 51s 2s/step - loss: 0.8709 - accuracy: 0.9305 - val_loss: 0.3936 - val_accuracy: 0.9681\n26/26 [==============================] - 35s 1s/step - loss: 0.3644 - accuracy: 0.9677 - val_loss: 0.3254 - val_accuracy: 0.9681\n26/26 [==============================] - 34s 1s/step - loss: 0.3392 - accuracy: 0.9677 - val_loss: 0.3178 - val_accuracy: 0.9681\n26/26 [==============================] - 41s 2s/step - loss: 0.3079 - accuracy: 0.9677 - val_loss: 0.2791 - val_accuracy: 0.9681\n26/26 [==============================] - 34s 1s/step - loss: 0.2876 - accuracy: 0.9678 - val_loss: 0.2686 - val_accuracy: 0.9681\n26/26 [==============================] - 34s 1s/step - loss: 0.2743 - accuracy: 0.9677 - val_loss: 0.2610 - val_accuracy: 0.9681\n26/26 [==============================] - 34s 1s/step - loss: 0.2729 - accuracy: 0.9678 - val_loss: 0.2637 - val_accuracy: 0.9681\n26/26 [==============================] - 34s 1s/step - loss: 0.2593 - accuracy: 0.9678 - val_loss: 0.2510 - val_accuracy: 0.9681\n26/26 [==============================] - 34s 1s/step - loss: 0.2237 - accuracy: 0.9678 - val_loss: 0.1839 - val_accuracy: 0.9682\n26/26 [==============================] - 38s 1s/step - loss: 0.1789 - accuracy: 0.9678 - val_loss: 0.1634 - val_accuracy: 0.9682\n26/26 [==============================] - 34s 1s/step - loss: 0.1637 - accuracy: 0.9679 - val_loss: 0.1553 - val_accuracy: 0.9682\n26/26 [==============================] - 34s 1s/step - loss: 0.1543 - accuracy: 0.9679 - val_loss: 0.1535 - val_accuracy: 0.9683\n26/26 [==============================] - 34s 1s/step - loss: 0.1488 - accuracy: 0.9680 - val_loss: 0.1493 - val_accuracy: 0.9684\n26/26 [==============================] - 34s 1s/step - loss: 0.1388 - accuracy: 0.9681 - val_loss: 0.1600 - val_accuracy: 0.9686\n26/26 [==============================] - 34s 1s/step - loss: 0.1373 - accuracy: 0.9682 - val_loss: 0.1379 - val_accuracy: 0.9688\n26/26 [==============================] - 34s 1s/step - loss: 0.1311 - accuracy: 0.9682 - val_loss: 0.1369 - val_accuracy: 0.9686\n26/26 [==============================] - 34s 1s/step - loss: 0.1230 - accuracy: 0.9683 - val_loss: 0.1287 - val_accuracy: 0.9688\n26/26 [==============================] - 34s 1s/step - loss: 0.1177 - accuracy: 0.9685 - val_loss: 0.1299 - val_accuracy: 0.9688\n26/26 [==============================] - 34s 1s/step - loss: 0.1152 - accuracy: 0.9686 - val_loss: 0.1255 - val_accuracy: 0.9690\n26/26 [==============================] - 34s 1s/step - loss: 0.1128 - accuracy: 0.9687 - val_loss: 0.1241 - val_accuracy: 0.9692\n26/26 [==============================] - 34s 1s/step - loss: 0.1104 - accuracy: 0.9689 - val_loss: 0.1239 - val_accuracy: 0.9694\n26/26 [==============================] - 34s 1s/step - loss: 0.1097 - accuracy: 0.9693 - val_loss: 0.1231 - val_accuracy: 0.9693\n26/26 [==============================] - 34s 1s/step - loss: 0.1060 - accuracy: 0.9693 - val_loss: 0.1118 - val_accuracy: 0.9694\n26/26 [==============================] - 34s 1s/step - loss: 0.0945 - accuracy: 0.9693 - val_loss: 0.1049 - val_accuracy: 0.9698\n26/26 [==============================] - 34s 1s/step - loss: 0.0899 - accuracy: 0.9696 - val_loss: 0.1074 - val_accuracy: 0.9698\n","output_type":"stream"}]},{"cell_type":"code","source":"import spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\ntext = nlp('Hi, My name is Aman Kharwal \\n I am from India \\n I want to work with Google \\n Steve Jobs is My Inspiration')\ndisplacy.render(text, style = 'ent', jupyter=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}